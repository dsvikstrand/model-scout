{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Query: 'small vision transformer for 224x224 image classification that fits on a single GPU' (k=100, sort_by=similarity) ===\n",
      "\n",
      "[1] Remade-AI/Squish\n",
      "   summary   : This LoRA model, Squish Effect LoRA for Wan2.1 14B I2V 480p, transforms images into videos of being…\n",
      "   similarity: 0.3138512969017029\n",
      "   params    : 0\n",
      "   likes     : 47 downloads: 2427\n",
      "\n",
      "[2] QuantStack/Phantom_Wan_14B-GGUF\n",
      "   summary   : A quantized version of bytedance-research/Phantom, designed for use with ComfyUI and compatible…\n",
      "   similarity: 0.31362342834472656\n",
      "   params    : 0\n",
      "   likes     : 5 downloads: 1420\n",
      "\n",
      "[3] RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16\n",
      "   summary   : A quantized version of Llama-3.3-70B-Instruct, optimized for efficient deployment with vLLM and Red…\n",
      "   similarity: 0.3134850263595581\n",
      "   params    : 11200046176\n",
      "   likes     : 0 downloads: 13283\n",
      "\n",
      "[4] Mungert/olmOCR-7B-0225-preview-GGUF\n",
      "   summary   : This model is a LoFi, BF16-accelerated OCR model for use with GPUs and TPUs, offering a balance…\n",
      "   similarity: 0.3132011294364929\n",
      "   params    : 0\n",
      "   likes     : 1 downloads: 1935\n",
      "\n",
      "[5] mradermacher/Gemini-Distill-Qwen2.5-0.5B-ead-GGUF\n",
      "   summary   : A quantized version of Gemini-Distill-Qwen2.5-0.5B-ead, offering various quantization levels for…\n",
      "   similarity: 0.3129551410675049\n",
      "   params    : 0\n",
      "   likes     : 0 downloads: 1544\n",
      "\n",
      "[6] Metric-AI/ColQwenStella-2b-multilingual\n",
      "   summary   : ColQwenStella-2b-multilingual is a multilingual visual retrieval model combining Qwen2 Vision and…\n",
      "   similarity: 0.31280648708343506\n",
      "   params    : 0\n",
      "   likes     : 7 downloads: 486\n",
      "\n",
      "[7] mradermacher/Ichigo-llama3.1-s-base-v0.3-i1-GGUF\n",
      "   summary   : A quantized version of Menlo/Ichigo-llama3.1-s-base-v0.3, offering various quantization levels to…\n",
      "   similarity: 0.311415433883667\n",
      "   params    : 0\n",
      "   likes     : 1 downloads: 2322\n",
      "\n",
      "[8] mradermacher/Llama-3-Soliloquy-8B-v2-GGUF\n",
      "   summary   : A quantized version of Llama-3-Soliloquy-8B-v2, offering various quantization levels for efficient…\n",
      "   similarity: 0.31128889322280884\n",
      "   params    : 0\n",
      "   likes     : 7 downloads: 302\n",
      "\n",
      "[9] unsloth/Qwen2.5-VL-7B-Instruct-unsloth-bnb-4bit\n",
      "   summary   : Qwen2.5-VL-7B is a 7 billion parameter vision-language model excelling in understanding and…\n",
      "   similarity: 0.31126517057418823\n",
      "   params    : 5118193888\n",
      "   likes     : 29 downloads: 36246\n",
      "\n",
      "[10] THUDM/cogvlm2-llama3-caption\n",
      "   summary   : CogVLM2-Llama3-Caption is a video captioning model that generates textual descriptions from video…\n",
      "   similarity: 0.3103371858596802\n",
      "   params    : 12507532544\n",
      "   likes     : 98 downloads: 15867\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from textwrap import shorten\n",
    "\n",
    "BASE_URL = \"https://davanstrien-huggingface-datasets-search-v2.hf.space\"\n",
    "\n",
    "def search_models(\n",
    "    query: str,\n",
    "    k: int = 10,\n",
    "    sort_by: str = \"similarity\",\n",
    "    min_param_count: int = 0,\n",
    "    max_param_count: int | None = None,\n",
    "):\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"k\": k,\n",
    "        \"sort_by\": sort_by,\n",
    "        \"min_param_count\": min_param_count,\n",
    "    }\n",
    "    if max_param_count is not None:\n",
    "        params[\"max_param_count\"] = max_param_count\n",
    "\n",
    "    resp = requests.get(f\"{BASE_URL}/search/models\", params=params, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "\n",
    "    results = data.get(\"results\", [])\n",
    "    print(f\"\\n=== Query: {query!r} (k={k}, sort_by={sort_by}) ===\")\n",
    "    if not results:\n",
    "        print(\"No results.\")\n",
    "        return\n",
    "\n",
    "    for i, r in enumerate(results[::-1], start=1):\n",
    "        mid = r.get(\"model_id\") or r.get(\"id\")\n",
    "        summary = r.get(\"summary\") or r.get(\"description\") or \"\"\n",
    "        params_str = r.get(\"param_count\")\n",
    "        likes = r.get(\"likes\")\n",
    "        downloads = r.get(\"downloads\")\n",
    "        sim = r.get(\"similarity\")\n",
    "\n",
    "        print(f\"\\n[{i}] {mid}\")\n",
    "        print(\"   summary   :\", shorten(summary, width=100, placeholder=\"…\"))\n",
    "        print(\"   similarity:\", sim)\n",
    "        print(\"   params    :\", params_str)\n",
    "        print(\"   likes     :\", likes, \"downloads:\", downloads)\n",
    "        if i == 10:\n",
    "            break\n",
    "\n",
    "# Example queries:\n",
    "tests = [\n",
    "    \"small vision transformer for 224x224 image classification that fits on a single GPU\",\n",
    "    \"small image model for classifying cats and dogs\",\n",
    "    #\"embedding model for English RAG on technical documents\",\n",
    "    #\"multilingual text embedding model for European languages\",\n",
    "    #\"object detection model for real-time inference on common objects\",\n",
    "    #\"speech-to-text model for English audio with good accuracy\",\n",
    "]\n",
    "\n",
    "search_models(tests[0], k=100, sort_by=\"similarity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e6af92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (4.48.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.3.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.11.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (0.27.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (10.0.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.12.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (5.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.7.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.7.9)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Downloading sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
      "Installing collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-5.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b94cc7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476bafc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_key = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3da3a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catalog models_catalog_with_queries.json not found. Falling back to models_catalog.json.\n",
      "Loaded 1962 queries from models_catalog.json.\n",
      "Embedded batch 1/123\n",
      "Embedded batch 2/123\n",
      "Embedded batch 3/123\n",
      "Embedded batch 4/123\n",
      "Embedded batch 5/123\n",
      "Embedded batch 6/123\n",
      "Embedded batch 7/123\n",
      "Embedded batch 8/123\n",
      "Embedded batch 9/123\n",
      "Embedded batch 10/123\n",
      "Embedded batch 11/123\n",
      "Embedded batch 12/123\n",
      "Embedded batch 13/123\n",
      "Embedded batch 14/123\n",
      "Embedded batch 15/123\n",
      "Embedded batch 16/123\n",
      "Embedded batch 17/123\n",
      "Embedded batch 18/123\n",
      "Embedded batch 19/123\n",
      "Embedded batch 20/123\n",
      "Embedded batch 21/123\n",
      "Embedded batch 22/123\n",
      "Embedded batch 23/123\n",
      "Embedded batch 24/123\n",
      "Embedded batch 25/123\n",
      "Embedded batch 26/123\n",
      "Embedded batch 27/123\n",
      "Embedded batch 28/123\n",
      "Embedded batch 29/123\n",
      "Embedded batch 30/123\n",
      "Embedded batch 31/123\n",
      "Embedded batch 32/123\n",
      "Embedded batch 33/123\n",
      "Embedded batch 34/123\n",
      "Embedded batch 35/123\n",
      "Embedded batch 36/123\n",
      "Embedded batch 37/123\n",
      "Embedded batch 38/123\n",
      "Embedded batch 39/123\n",
      "Embedded batch 40/123\n",
      "Embedded batch 41/123\n",
      "Embedded batch 42/123\n",
      "Embedded batch 43/123\n",
      "Embedded batch 44/123\n",
      "Embedded batch 45/123\n",
      "Embedded batch 46/123\n",
      "Embedded batch 47/123\n",
      "Embedded batch 48/123\n",
      "Embedded batch 49/123\n",
      "Embedded batch 50/123\n",
      "Embedded batch 51/123\n",
      "Embedded batch 52/123\n",
      "Embedded batch 53/123\n",
      "Embedded batch 54/123\n",
      "Embedded batch 55/123\n",
      "Embedded batch 56/123\n",
      "Embedded batch 57/123\n",
      "Embedded batch 58/123\n",
      "Embedded batch 59/123\n",
      "Embedded batch 60/123\n",
      "Embedded batch 61/123\n",
      "Embedded batch 62/123\n",
      "Embedded batch 63/123\n",
      "Embedded batch 64/123\n",
      "Embedded batch 65/123\n",
      "Embedded batch 66/123\n",
      "Embedded batch 67/123\n",
      "Embedded batch 68/123\n",
      "Embedded batch 69/123\n",
      "Embedded batch 70/123\n",
      "Embedded batch 71/123\n",
      "Embedded batch 72/123\n",
      "Embedded batch 73/123\n",
      "Embedded batch 74/123\n",
      "Embedded batch 75/123\n",
      "Embedded batch 76/123\n",
      "Embedded batch 77/123\n",
      "Embedded batch 78/123\n",
      "Embedded batch 79/123\n",
      "Embedded batch 80/123\n",
      "Embedded batch 81/123\n",
      "Embedded batch 82/123\n",
      "Embedded batch 83/123\n",
      "Embedded batch 84/123\n",
      "Embedded batch 85/123\n",
      "Embedded batch 86/123\n",
      "Embedded batch 87/123\n",
      "Embedded batch 88/123\n",
      "Embedded batch 89/123\n",
      "Embedded batch 90/123\n",
      "Embedded batch 91/123\n",
      "Embedded batch 92/123\n",
      "Embedded batch 93/123\n",
      "Embedded batch 94/123\n",
      "Embedded batch 95/123\n",
      "Embedded batch 96/123\n",
      "Embedded batch 97/123\n",
      "Embedded batch 98/123\n",
      "Embedded batch 99/123\n",
      "Embedded batch 100/123\n",
      "Embedded batch 101/123\n",
      "Embedded batch 102/123\n",
      "Embedded batch 103/123\n",
      "Embedded batch 104/123\n",
      "Embedded batch 105/123\n",
      "Embedded batch 106/123\n",
      "Embedded batch 107/123\n",
      "Embedded batch 108/123\n",
      "Embedded batch 109/123\n",
      "Embedded batch 110/123\n",
      "Embedded batch 111/123\n",
      "Embedded batch 112/123\n",
      "Embedded batch 113/123\n",
      "Embedded batch 114/123\n",
      "Embedded batch 115/123\n",
      "Embedded batch 116/123\n",
      "Embedded batch 117/123\n",
      "Embedded batch 118/123\n",
      "Embedded batch 119/123\n",
      "Embedded batch 120/123\n",
      "Embedded batch 121/123\n",
      "Embedded batch 122/123\n",
      "Embedded batch 123/123\n",
      "Final embedding matrix shape: (1962, 384)\n",
      "Saved embeddings to data\\query_embeddings.npy\n",
      "Saved metadata to data\\query_meta.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "API_URL = (\n",
    "    \"https://router.huggingface.co/hf-inference/models/\"\n",
    "    \"BAAI/bge-small-en-v1.5/pipeline/feature-extraction\"\n",
    ")\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "def load_catalog(path: Path) -> List[Dict[str, Any]]:\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def flatten_queries(catalog: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    for model in catalog:\n",
    "        model_id = model.get(\"id\")\n",
    "        queries_by_level = model.get(\"queries_by_level\") or {}\n",
    "        for level in (\"expert\", \"junior\", \"beginner\"):\n",
    "            for query in queries_by_level.get(level, []) or []:\n",
    "                if not query:\n",
    "                    continue\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"model_id\": model_id,\n",
    "                        \"level\": level,\n",
    "                        \"query\": query,\n",
    "                    }\n",
    "                )\n",
    "    return rows\n",
    "\n",
    "\n",
    "def embed_batch(texts: List[str], token: str) -> np.ndarray:\n",
    "    if not token:\n",
    "        raise RuntimeError(\"HF_TOKEN is required to call the Inference API\")\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "    }\n",
    "    resp = requests.post(\n",
    "        API_URL,\n",
    "        headers=headers,\n",
    "        json={\"inputs\": texts},\n",
    "        timeout=60,\n",
    "    )\n",
    "    if not resp.ok:\n",
    "        raise RuntimeError(\n",
    "            f\"Inference API error {resp.status_code}: {resp.text}\"\n",
    "        )\n",
    "\n",
    "    data = resp.json()\n",
    "    # Expect shape [batch, dim]\n",
    "    try:\n",
    "        arr = np.array(data, dtype=np.float32)\n",
    "    except Exception as exc:  # noqa: BLE001\n",
    "        raise RuntimeError(f\"Unexpected embedding response payload: {data}\") from exc\n",
    "\n",
    "    if arr.ndim != 2 or arr.shape[0] != len(texts):\n",
    "        raise RuntimeError(\n",
    "            f\"Embedding batch shape mismatch: got {arr.shape}, \"\n",
    "            f\"expected ({len(texts)}, D)\"\n",
    "        )\n",
    "\n",
    "    # L2-normalize per vector\n",
    "    norms = np.linalg.norm(arr, axis=1, keepdims=True)\n",
    "    arr = arr / np.clip(norms, 1e-12, None)\n",
    "    return arr.astype(np.float32)\n",
    "\n",
    "\n",
    "def build_embeddings(rows: List[Dict[str, Any]], token: str) -> np.ndarray:\n",
    "    vectors: List[np.ndarray] = []\n",
    "    total_batches = (len(rows) - 1) // BATCH_SIZE + 1\n",
    "\n",
    "    for i in range(0, len(rows), BATCH_SIZE):\n",
    "        batch = rows[i : i + BATCH_SIZE]\n",
    "        texts = [r[\"query\"] for r in batch]\n",
    "        vecs = embed_batch(texts, token)\n",
    "        vectors.append(vecs)\n",
    "        print(f\"Embedded batch {i // BATCH_SIZE + 1}/{total_batches}\")\n",
    "\n",
    "    return np.vstack(vectors) if vectors else np.zeros((0, 0), dtype=np.float32)\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    token = hf_key\n",
    "    if not token:\n",
    "        raise SystemExit(\"HF_TOKEN is not set in the environment.\")\n",
    "\n",
    "    catalog_path = Path(\"models_catalog_with_queries.json\")\n",
    "    if not catalog_path.exists():\n",
    "        fallback = Path(\"models_catalog.json\")\n",
    "        if fallback.exists():\n",
    "            print(f\"Catalog {catalog_path} not found. Falling back to {fallback}.\")\n",
    "            catalog_path = fallback\n",
    "        else:\n",
    "            raise SystemExit(\n",
    "                f\"Catalog file not found: {catalog_path} or {fallback}\"\n",
    "            )\n",
    "\n",
    "    catalog = load_catalog(catalog_path)\n",
    "    rows = flatten_queries(catalog)\n",
    "    if not rows:\n",
    "        raise SystemExit(\"No queries found in catalog; nothing to embed.\")\n",
    "\n",
    "    print(f\"Loaded {len(rows)} queries from {catalog_path}.\")\n",
    "    embeddings = build_embeddings(rows, token)\n",
    "    print(f\"Final embedding matrix shape: {embeddings.shape}\")\n",
    "\n",
    "    output_dir = Path(\"data\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    embeddings_path = output_dir / \"query_embeddings.npy\"\n",
    "    meta_path = output_dir / \"query_meta.json\"\n",
    "\n",
    "    np.save(embeddings_path, embeddings)\n",
    "    with meta_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(rows, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Saved embeddings to {embeddings_path}\")\n",
    "    print(f\"Saved metadata to {meta_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e9edf00",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "InferenceClient.__init__() got an unexpected keyword argument 'provider'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[31], line 83\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mSystemExit\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHF_TOKEN is not set in the environment.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# IMPORTANT: force the client to use the new router base URL\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mInferenceClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhf-inference\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m catalog_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels_catalog_with_queries.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m catalog_path\u001b[38;5;241m.\u001b[39mexists():\n",
      "\u001b[1;31mTypeError\u001b[0m: InferenceClient.__init__() got an unexpected keyword argument 'provider'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
